# Como Adicionar Novos Provedores de LLM ao Educare+

Este guia explica o passo a passo para integrar novos provedores de LLM (como Claude, Llama, etc) √† plataforma Educare+.

---

## üìã Vis√£o Geral da Arquitetura

O sistema de LLM do Educare+ √© dividido em 4 camadas:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. FRONTEND (PromptManagement.tsx)                          ‚îÇ
‚îÇ    - Seleciona Provedor + Modelo                            ‚îÇ
‚îÇ    - Ajusta Temperatura e Max Tokens                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. API REST (llmConfigController.js)                        ‚îÇ
‚îÇ    - PUT /api/llm-configs/:module_type                      ‚îÇ
‚îÇ    - GET /api/llm-configs/providers                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. BANCO DE DADOS (AssistantLLMConfig)                      ‚îÇ
‚îÇ    - Persiste provider, model_name, temperature, max_tokens ‚îÇ
‚îÇ    - Cache de 5 minutos para performance                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. LLM PROVIDER REGISTRY (llmProviderRegistry.js)           ‚îÇ
‚îÇ    - Carrega config do banco                                ‚îÇ
‚îÇ    - Chama o provedor correto (OpenAI, Gemini, etc)        ‚îÇ
‚îÇ    - Retorna resposta para o RAG                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Passo a Passo para Adicionar um Novo Provedor

### **Passo 1: Configurar a Vari√°vel de Ambiente**

Toda API precisa de uma chave de autentica√ß√£o. Adicione a vari√°vel no seu `.env`:

```bash
# .env (desenvolvimento)
NOVO_PROVEDOR_API_KEY=sk-xxx-seu-token-aqui

# Ou solicite via Replit Secrets (produ√ß√£o)
# Replit GUI ‚Üí Secrets ‚Üí Adicionar NOVO_PROVEDOR_API_KEY
```

**Exemplos de provedores reais:**
```bash
OPENAI_API_KEY=sk-proj-xxx
GEMINI_API_KEY=AIzaSyxxx
ANTHROPIC_API_KEY=sk-ant-xxx
GROQ_API_KEY=gsk_xxx
DEEPSEEK_API_KEY=sk-xxx
```

### **Passo 2: Registrar o Provedor no Registry**

Edite `educare-backend/src/services/llmProviderRegistry.js` e adicione sua entrada na const `LLM_PROVIDERS`:

```javascript
// Dentro de const LLM_PROVIDERS = { ... }

novo_provedor: {
  id: 'novo_provedor',                              // ID √∫nico (sem espa√ßos/caracteres especiais)
  name: 'Nome do Provedor',                         // Nome exibido na UI
  type: 'openai-compatible',                        // Tipo: 'openai-compatible', 'gemini', 'anthropic'
  envKey: 'NOVO_PROVEDOR_API_KEY',                  // Chave de env
  baseUrl: 'https://api.novo-provedor.com/v1',     // URL base (deixe null para Custom)
  models: [
    {
      id: 'model-id-1',                             // ID que ser√° enviado √† API
      name: 'Model Display Name',                   // Nome exibido na UI
      description: 'Breve descri√ß√£o do modelo',     // Tooltip na UI
      context_window: 128000                        // Janela de contexto (tokens)
    },
    {
      id: 'model-id-2',
      name: 'Outro Modelo',
      description: 'Descri√ß√£o...',
      context_window: 64000
    }
  ]
}
```

**Exemplo Pr√°tico (Anthropic Claude):**
```javascript
anthropic: {
  id: 'anthropic',
  name: 'Anthropic (Claude)',
  type: 'anthropic',                                // Tipo especial (requer callAnthropic)
  envKey: 'ANTHROPIC_API_KEY',
  baseUrl: 'https://api.anthropic.com/v1',
  models: [
    {
      id: 'claude-3-5-sonnet-20241022',
      name: 'Claude 3.5 Sonnet',
      description: 'Equil√≠brio entre velocidade e capacidade',
      context_window: 200000
    }
  ]
}
```

### **Passo 3: Implementar o M√©todo de Chamada**

Adicione um m√©todo na classe `LLMProviderRegistry` para chamar sua API. Escolha conforme o tipo:

#### **Op√ß√£o A: Compat√≠vel com OpenAI** 
Se sua API usa o mesmo formato da OpenAI (request/response), use:

```javascript
// J√° existe: callOpenAICompatible()
// Funciona para: OpenAI, DeepSeek, Groq, Together, xAI, OpenRouter, Custom

// Apenas certifique-se de que:
// 1. type: 'openai-compatible'
// 2. baseUrl est√° correto
// 3. Env key est√° definida
```

#### **Op√ß√£o B: Implementar M√©todo Customizado**

Se a API √© diferente (como Gemini ou Claude), implemente um novo m√©todo:

```javascript
// Adicione ao final da classe LLMProviderRegistry

async callNovoProvedor(config, messages) {
  const { model_name, temperature, max_tokens, additional_params } = config;
  const apiKey = additional_params?.api_key || this.getApiKey('novo_provedor');
  
  if (!apiKey) {
    throw new Error('NOVO_PROVEDOR_API_KEY n√£o configurada');
  }

  // 1. Extrair system message (se houver)
  const systemMessage = messages.find(m => m.role === 'system');
  const chatMessages = messages.filter(m => m.role !== 'system');

  // 2. Fazer request para a API
  const response = await fetch('https://api.novo-provedor.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: model_name,
      max_tokens,
      temperature,
      system: systemMessage?.content,
      messages: chatMessages.map(m => ({
        role: m.role,
        content: m.content
      }))
    })
  });

  if (!response.ok) {
    const error = await response.json();
    throw new Error(`Novo Provedor API error: ${error.error?.message || response.statusText}`);
  }

  const data = await response.json();

  // 3. Retornar no formato padr√£o Educare
  return {
    content: data.content[0]?.text || '',           // Texto da resposta
    usage: {
      prompt_tokens: data.usage?.input_tokens,
      completion_tokens: data.usage?.output_tokens,
      total_tokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
    },
    model: model_name,
    provider: 'novo_provedor'
  };
}
```

### **Passo 4: Registrar o M√©todo no `callLLM()`**

Adicione um case no m√©todo principal:

```javascript
async callLLM(config, messages) {
  const { provider, model_name, temperature, max_tokens, additional_params } = config;
  const providerInfo = this.getProvider(provider);
  
  if (!providerInfo) {
    throw new Error(`Provider '${provider}' n√£o encontrado`);
  }
  
  if (!this.isProviderAvailable(provider) && provider !== 'custom') {
    throw new Error(`Provider '${provider}' n√£o est√° dispon√≠vel. Configure a vari√°vel ${providerInfo.envKey}`);
  }

  switch (providerInfo.type) {
    case 'openai-compatible':
      return this.callOpenAICompatible(providerInfo, config, messages);
    case 'gemini':
      return this.callGemini(config, messages);
    case 'anthropic':
      return this.callAnthropic(config, messages);
    case 'novo_provedor':  // ‚Üê ADICIONE AQUI
      return this.callNovoProvedor(config, messages);
    default:
      return this.callOpenAICompatible(providerInfo, config, messages);
  }
}
```

### **Passo 5: Atualizar o Frontend (TypeScript)**

No arquivo `src/services/api/llmConfigService.ts`, adicione o novo tipo de provedor:

```typescript
export type ProviderType = 'openai' | 'gemini' | 'deepseek' | 'groq' | 'xai' | 'anthropic' | 'together' | 'openrouter' | 'novo_provedor' | 'custom';
```

### **Passo 6: Testar o Novo Provedor**

**Teste 1: Verificar Disponibilidade**
```bash
cd educare-backend

node -e "
const { providerRegistry } = require('./src/services/llmProviderRegistry');
const providers = providerRegistry.getAvailableProviders();
const novo = providers.find(p => p.id === 'novo_provedor');
console.log('Novo Provedor:', novo);
"
```

**Teste 2: Testar Chamada Completa**
```bash
node -e "
const llmConfigService = require('./src/services/llmConfigService');

(async () => {
  // Atualizar config para usar novo provedor
  await llmConfigService.updateConfig('baby', {
    provider: 'novo_provedor',
    model_name: 'model-id-1',
    temperature: 0.7,
    max_tokens: 1500
  });

  // Carregar e verificar
  const config = await llmConfigService.getConfig('baby');
  console.log('Config atualizada:', config);
})();
"
```

**Teste 3: Teste End-to-End**
```bash
# 1. Abra o navegador em /educare-app/owner/prompt-management
# 2. Fa√ßa login como Owner
# 3. Clique em "Configura√ß√µes do Modelo" (se√ß√£o TitiNauta)
# 4. Verifique se "Novo Provedor" aparece na lista
# 5. Selecione-o e veja se os modelos aparecem
# 6. Salve a configura√ß√£o
# 7. Verifique nos logs do backend se a configura√ß√£o foi salva
```

---

## üìö Exemplos de Provedores Implementados

### **OpenAI (OpenAI-Compatible)**
```javascript
openai: {
  id: 'openai',
  name: 'OpenAI',
  type: 'openai-compatible',  // ‚Üê Usa callOpenAICompatible
  envKey: 'OPENAI_API_KEY',
  baseUrl: 'https://api.openai.com/v1',
  models: [
    { id: 'gpt-4o-mini', name: 'GPT-4o Mini', ... },
    { id: 'gpt-4o', name: 'GPT-4o', ... }
  ]
}
```

### **Google Gemini (Customizado)**
```javascript
gemini: {
  id: 'gemini',
  name: 'Google Gemini',
  type: 'gemini',              // ‚Üê Tem callGemini pr√≥prio
  envKey: 'GEMINI_API_KEY',
  models: [
    { id: 'gemini-2.0-flash', name: 'Gemini 2.0 Flash', ... }
  ]
}
// Implementa√ß√£o: callGemini() usa GoogleGenerativeAI SDK
```

### **Anthropic Claude (Customizado)**
```javascript
anthropic: {
  id: 'anthropic',
  name: 'Anthropic (Claude)',
  type: 'anthropic',           // ‚Üê Tem callAnthropic pr√≥prio
  envKey: 'ANTHROPIC_API_KEY',
  baseUrl: 'https://api.anthropic.com/v1',
  models: [
    { id: 'claude-3-5-sonnet-20241022', name: 'Claude 3.5 Sonnet', ... }
  ]
}
// Implementa√ß√£o: callAnthropic() usa fetch direto (n√£o tem SDK oficial em Node)
```

### **Custom (OpenAI-Compatible)**
```javascript
custom: {
  id: 'custom',
  name: 'Custom OpenAI-Compatible',
  type: 'openai-compatible',   // ‚Üê Usa callOpenAICompatible
  envKey: 'CUSTOM_LLM_API_KEY',
  baseUrl: null,               // ‚Üê Vem do additional_params.base_url
  models: [
    { id: 'custom-model', name: 'Custom Model', ... }
  ]
}
```

---

## üîë Vari√°veis de Ambiente Recomendadas

| Provedor | Env Key | Exemplo | Docs |
|----------|---------|---------|------|
| OpenAI | `OPENAI_API_KEY` | `sk-proj-xxx` | https://platform.openai.com/api-keys |
| Google Gemini | `GEMINI_API_KEY` | `AIzaSyxxx` | https://aistudio.google.com/app/apikey |
| Anthropic Claude | `ANTHROPIC_API_KEY` | `sk-ant-xxx` | https://console.anthropic.com |
| Groq | `GROQ_API_KEY` | `gsk_xxx` | https://console.groq.com |
| DeepSeek | `DEEPSEEK_API_KEY` | `sk-xxx` | https://platform.deepseek.com |
| Together AI | `TOGETHER_API_KEY` | `xxx` | https://api.together.xyz |
| xAI Grok | `XAI_API_KEY` | `xxx` | https://console.x.ai |
| OpenRouter | `OPENROUTER_API_KEY` | `sk-or-xxx` | https://openrouter.ai/keys |

---

## üîÑ Fluxo de Dados (Exemplo Pr√°tico)

**Cen√°rio:** Owner seleciona "Claude 3.5 Sonnet" para TitiNauta Materna

```
1. FRONTEND (PromptManagement.tsx)
   ‚îú‚îÄ Seleciona provider: "anthropic"
   ‚îú‚îÄ Seleciona model: "claude-3-5-sonnet-20241022"
   ‚îú‚îÄ Define temperature: 0.8
   ‚îú‚îÄ Define max_tokens: 2000
   ‚îî‚îÄ PUT /api/llm-configs/mother
       { provider: "anthropic", model_name: "claude-3-5-sonnet-20241022", ... }

2. BACKEND API (llmConfigController)
   ‚îú‚îÄ Recebe request
   ‚îú‚îÄ Valida se anthropic_api_key existe
   ‚îî‚îÄ Chama llmConfigService.updateConfig()

3. BANCO DE DADOS
   ‚îú‚îÄ INSERT/UPDATE assistant_llm_configs
   ‚îÇ  SET provider='anthropic', model_name='claude-3-5-sonnet-20241022', ...
   ‚îÇ  WHERE module_type='mother'
   ‚îî‚îÄ Retorna config salva

4. FRONTEND (recebe resposta)
   ‚îî‚îÄ Toast de sucesso: "Configura√ß√£o salva!"

5. DURANTE CONVERSA (RAG usa config)
   ‚îú‚îÄ Usu√°rio: "Qual √© a melhor posi√ß√£o para dormir?"
   ‚îú‚îÄ RAG carrega config da madre
   ‚îÇ  { provider: 'anthropic', model_name: 'claude-3-5-sonnet-20241022', ... }
   ‚îú‚îÄ llmProviderRegistry.callLLM(config, messages)
   ‚îú‚îÄ Identifica type='anthropic'
   ‚îú‚îÄ Chama callAnthropic()
   ‚îú‚îÄ Faz request para https://api.anthropic.com/v1/messages
   ‚îî‚îÄ Retorna resposta ao usu√°rio
```

---

## ‚ö†Ô∏è Troubleshooting

### Provedor n√£o aparece na UI
- [ ] Verificou se a `envKey` est√° definida em `.env`?
- [ ] Restart backend e frontend?
- [ ] Cheque console logs: `providerRegistry.getAvailableProviders()`

### Erro "API key n√£o configurada"
```bash
# Solu√ß√£o 1: Adicionar ao .env
NOVO_PROVEDOR_API_KEY=sua-chave-aqui

# Solu√ß√£o 2: Se em produ√ß√£o, usar Replit Secrets
# Replit GUI ‚Üí Secrets ‚Üí Adicionar nova vari√°vel
```

### Modelo n√£o retorna respostas corretas
- [ ] Verifique o formato de request da API
- [ ] Confira se o `model_name` existe no provedor
- [ ] Monitore os logs: `console.log()` no m√©todo `callNovoProvedor()`

### Cache n√£o invalida ap√≥s update
```javascript
// O sistema j√° invalida automaticamente:
llmConfigService.invalidateCache('baby');  // Invoca ao atualizar
llmConfigService.invalidateAllCaches();    // Limpa tudo
```

---

## üìñ Leitura Recomendada

- **LLM Provider Registry**: `educare-backend/src/services/llmProviderRegistry.js`
- **LLM Config Service**: `educare-backend/src/services/llmConfigService.js`
- **LLM Config Controller**: `educare-backend/src/controllers/llmConfigController.js`
- **Prompt Management UI**: `src/pages/admin/PromptManagement.tsx`
- **RAG Service (integra√ß√£o)**: `educare-backend/src/services/ragService.js`

---

## ‚úÖ Checklist de Implementa√ß√£o

- [ ] Criar `NOVO_PROVEDOR_API_KEY` no `.env`
- [ ] Adicionar provedor em `LLM_PROVIDERS` (llmProviderRegistry.js)
- [ ] Implementar m√©todo `callNovoProvedor()` se necess√°rio
- [ ] Adicionar case no `callLLM()` se type for customizado
- [ ] Atualizar `ProviderType` no frontend (llmConfigService.ts)
- [ ] Testar disponibilidade com script Node
- [ ] Testar sele√ß√£o na UI (PromptManagement.tsx)
- [ ] Testar salvamento no banco de dados
- [ ] Testar chamada real da API durante chat
- [ ] Documentar os models suportados
- [ ] Adicionar ao README/documenta√ß√£o do projeto

---

D√∫vidas? Consulte os exemplos de OpenAI, Gemini e Anthropic no c√≥digo-fonte!
