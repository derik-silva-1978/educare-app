# FASE 3 ‚Äî PROMPT PARA O REPLIT (BACKEND)
## Objetivo: Implementar o n√∫cleo do RAG (Consulta), sem alterar fluxos existentes.  
### Nesta fase voc√™ vai criar **somente a infraestrutura de consulta**:
- servi√ßo RAG
- integra√ß√£o com PostgreSQL para sele√ß√£o de documentos
- integra√ß√£o com File Search para recuperar trechos
- montagem de prompt para o LLM (Gemini e/ou OpenAI)
- endpoint `/rag/ask` seguro e compat√≠vel com o fluxo atual do Educare App

‚ö†Ô∏è IMPORTANTE:  
Nesta fase **n√£o haver√° ainda personaliza√ß√£o profunda do beb√™**, que vir√° na Fase 4.  
O objetivo √© garantir que a pipeline RAG responda corretamente e de forma est√°vel.

---

# ‚úîÔ∏è 1. CRIAR O M√ìDULO `ragService`
Local correto deve seguir o padr√£o do projeto, por exemplo:

src/services/ragService.js

ou equivalente em Python, se o backend for Python.

O m√≥dulo deve conter:

## 1.1. Fun√ß√£o: `selectKnowledgeDocuments(queryFilters)`
Objetivo: Selecionar quais documentos (‚Äúknowledge_documents‚Äù) ser√£o enviados ao File Search.

### Deve:
- receber objeto com filtros como:
  - `age_range`
  - `domain`
  - `tags`
- construir uma query SQL segura usando o padr√£o atual do projeto
- retornar lista de:
  - `id`
  - `title`
  - `file_search_id`
  - `tags`
  - `age_range`
  - `domain`

### Regras:
- Nunca alterar a tabela.
- N√£o usar SELECT *, apenas campos necess√°rios.
- Usar par√¢metros preparados.

---

## 1.2. Fun√ß√£o: `retrieveFromFileSearch(question, fileSearchIds)`
Objetivo: Consultar a API do File Search com base nos documentos filtrados.

### Deve:
1. Receber:
   - a pergunta do usu√°rio
   - lista de `file_search_id`
2. Montar a payload da API:
   - query = pergunta
   - documents = lista de IDs filtrados
3. Chamar o servi√ßo `fileSearchService` (criado na Fase 2)
4. Retornar lista de trechos relevantes:
   - texto recuperado
   - refer√™ncia ao documento
   - score (se fornecido pela API)

### Regras:
- Se nenhum documento for encontrado ‚Üí retornar array vazio.
- Se a API retornar erro ‚Üí logar e retornar array vazio (n√£o quebrar backend).

---

## 1.3. Fun√ß√£o: `buildLLMPrompt(question, retrievedChunks)`
Objetivo: Preparar o prompt a ser enviado ao LLM.

O prompt deve conter:
- ‚Äúinstru√ß√µes do sistema‚Äù (vers√£o inicial do TitiNauta ‚Äî voc√™ ainda vai refinar na fase 4)
- pergunta original do usu√°rio
- trechos recuperados anotados como cita√ß√µes
- instru√ß√µes de seguran√ßa:
  - ‚Äún√£o inventar textos‚Äù
  - ‚Äúuse apenas os trechos recuperados‚Äù
  - ‚Äúcite fonte interna quando relevante‚Äù

### Exemplo de estrutura:

SYSTEM:
Voc√™ √© o assistente Educare App. Responda de forma clara, acolhedora e sem alucinar.
Use EXCLUSIVAMENTE os trechos fornecidos pelo mecanismo de busca.
Cite sempre a origem do trecho se poss√≠vel.

QUESTION:
{{pergunta original}}

CONTEXT EXCERPTS:
  1.	{{trecho A}}
  2.	{{trecho B}}

RULES:
  ‚Ä¢	Se n√£o houver informa√ß√µes suficientes, diga que n√£o foi poss√≠vel encontrar no material oficial.
  ‚Ä¢	N√£o invente dados m√©dicos ou recomenda√ß√µes cl√≠nicas.

Replit deve ajustar o texto ao padr√£o do sistema.

---

## 1.4. Fun√ß√£o: `callLLM(prompt)`
Objetivo: Chamar Gemini ou OpenAI baseada em `.env`

### Deve:
- Detectar qual LLM est√° ativa (Gemini ou GPT-4.x)  
- Enviar o prompt completo
- Tratar:
  - erros de API
  - timeouts
  - respostas inv√°lidas
- Retornar apenas o texto gerado pela LLM

---

## 1.5. Fun√ß√£o principal: `runRAG(question, queryFilters)`
Juntando tudo:

1. Selecionar documentos no PostgreSQL  
   `docs = await selectKnowledgeDocuments(filters)`

2. Extrair file_search_ids  
   `ids = docs.map(d => d.file_search_id)`

3. Chamar File Search  
   `chunks = await retrieveFromFileSearch(question, ids)`

4. Construir prompt do LLM  
   `prompt = buildLLMPrompt(question, chunks)`

5. Executar LLM  
   `response = await callLLM(prompt)`

6. Retornar JSON:

{
‚Äúanswer‚Äù: ‚Äú‚Ä¶‚Äù,
‚Äúcitations‚Äù: [‚Ä¶],
‚Äúdocuments_used‚Äù: [‚Ä¶],
‚Äúchunks_used‚Äù: [‚Ä¶]
}

---

# ‚úîÔ∏è 2. IMPLEMENTAR ROTA `/rag/ask` (SEM MEXER NO n8n)

### Rota:

POST /rag/ask

### Entrada:

{
‚Äúquestion‚Äù: ‚Äútexto da pergunta‚Äù,
‚Äúfilters‚Äù: {
‚Äúage_range‚Äù: ‚Äú0-3m‚Äù,
‚Äúdomain‚Äù: ‚Äúmotor‚Äù,
‚Äútags‚Äù: [‚Äúcontrole_cef√°lico‚Äù]
}
}

### A rota deve:

1. Validar entrada (question obrigat√≥ria).
2. Sanitizar campos (evitar injection).
3. Chamar `ragService.runRAG(question, filters)`.
4. Retornar resposta estruturada.
5. Logar a opera√ß√£o (uso de LLM, filtros e documentos selecionados).
6. Garantir que erros:
   - sejam logados
   - N√ÉO quebrem o servidor
   - retornem status adequado (500, 400 etc.)

> ‚ö†Ô∏è Nesta fase, N√ÉO incluir l√≥gica de personaliza√ß√£o do beb√™.  
Essa l√≥gica entrar√° na Fase 4.

---

# ‚úîÔ∏è 3. DOCUMENTA√á√ÉO NECESS√ÅRIA (ATUALIZAR)

Atualizar `docs/RAG-EDUCARE.md` com:

### 3.1. Estrutura do m√≥dulo RAG (arquivo, fun√ß√µes, assinatura)  
### 3.2. Descri√ß√£o do endpoint `/rag/ask`  
- par√¢metros  
- exemplo de request  
- exemplo de response  
- limita√ß√µes da fase atual  

### 3.3. Como configurar LLM no `.env`  

LLM_PROVIDER=gemini|openai
GEMINI_API_KEY=‚Ä¶
OPENAI_API_KEY=‚Ä¶

---

# ‚úîÔ∏è 4. REGRAS DE SEGURAN√áA DA FASE 3

- N√£o alterar rotas existentes.
- N√£o alterar fluxos existentes do n8n.
- N√£o usar tabelas existentes para armazenar dados do RAG.
- N√£o vazar dados sens√≠veis para prompts do LLM.
- Nunca permitir que erros de File Search derrubem o backend.
- Sempre responder com JSON consistente.

---

# üìå SA√çDA ESPERADA DA FASE 3

- M√≥dulo `ragService` criado com todas as fun√ß√µes descritas.  
- Servi√ßo File Search sendo chamado corretamente.  
- Endpoint `/rag/ask` funcional e seguro.  
- Documenta√ß√£o atualizada.  
- Todo o sistema rodando SEM quebrar funcionalidades j√° existentes.