# FASE 5 ‚Äî PROMPT PARA O REPLIT (BACKEND)
## Objetivo: Implementar MECANISMOS DE OTIMIZA√á√ÉO, CACHE, CONTROLE DE CUSTO, FALLBACK ENTRE LLMs, E HARDENING DO RAG.
### Nesta fase voc√™ aprimora o sistema para produ√ß√£o:
- mais r√°pido,
- mais barato,
- mais seguro,
- mais est√°vel,
- mais resiliente,
- sem risco de quebrar o backend ou o fluxo n8n.

Nada desta fase deve quebrar implementa√ß√µes anteriores.  
Nada deve alterar tabelas ou fluxos de banco.  
Apenas funcionalidades adicionais e seguras.

---

# ‚úîÔ∏è 1. IMPLEMENTAR CACHE DE RESULTADOS DE FILE SEARCH E DE RESPOSTAS DO RAG

### 1.1. Criar m√≥dulo de cache
Seguindo o padr√£o do projeto, por exemplo:

src/services/cacheService.js

Use a melhor tecnologia dispon√≠vel no projeto:
- se o projeto j√° usa Redis ‚Üí usar Redis  
- se n√£o houver Redis ‚Üí usar cache in-memory com expira√ß√£o configur√°vel  
- nunca bloquear execu√ß√£o caso o cache falhe

### 1.2. Estrat√©gia de Cache

#### Cache de File Search (curto prazo)
Chave:

fileSearch:{hash( question + ids )}

TTL:
- 2 a 10 minutos (ajustar conforme desempenho real)

Armazena:
- chunks recuperados do File Search

#### Cache de resposta final (curto ou m√©dio prazo)
Chave:

ragResponse:{hash(question + babyContext + filters)}

TTL:
- 5 a 30 minutos

Armazena:
- a resposta final enviada ao usu√°rio

‚ö†Ô∏è Regras:
- o cache deve ser ignorado se o tamanho da resposta ultrapassar limites
- o cache nunca deve imped√≠r uma nova consulta se estiver inv√°lido

---

# ‚úîÔ∏è 2. IMPLEMENTAR MECANISMO DE FALLBACK ENTRE LLMs (GEMINI ‚Üí OPENAI)

### 2.1. Estrat√©gia
Se a chamada Gemini falhar por:
- timeout  
- erro 500  
- erro na API do File Search  
- indisponibilidade tempor√°ria  

Ent√£o:
- automaticamente tentar o GPT-4.1 (ou o modelo definido no `.env`)

### 2.2. Estrutura recomendada (adaptar ao projeto)

try {
return callGemini(prompt)
} catch {
log(‚ÄúLLM Gemini falhou, utilizando fallback OpenAI‚Äù)
return callOpenAI(prompt)
}

### 2.3. Regras de seguran√ßa
- qualquer exce√ß√£o deve ser logada  
- fallback n√£o pode travar o backend  
- fallback deve ser transparente para o usu√°rio final  

---

# ‚úîÔ∏è 3. IMPLEMENTAR LIMITE DE TOKENS E DE CUSTO POR REQUISI√á√ÉO

Criar m√≥dulo:

src/services/usageGuardService.js

Com funcionalidades:

### 3.1. `estimatePromptCost(prompt)`
- contar tokens se poss√≠vel (ou estimar por tamanho)
- logar volume de tokens enviados

### 3.2. Regras de prote√ß√£o
- se o prompt passar de um limite (ex.: 4096 tokens) ‚Üí truncar contexto com seguran√ßa
- se o custo estimado passar de um limite ‚Üí usar uma vers√£o mais barata da LLM (ex.: Gemini Flash)

### 3.3. Logar tudo
- n√∫mero de tokens enviados  
- modelo usado  
- fallback ativado ou n√£o  
- custo estimado  

---

# ‚úîÔ∏è 4. PROTE√á√ÉO CONTRA ABUSO E PROMPT INJECTION

Adicionar prote√ß√£o no servi√ßo que recebe a pergunta.

### 4.1. Fun√ß√£o de Sanitiza√ß√£o
Criar m√≥dulo:

src/utils/sanitizeUserPrompt.js

Fun√ß√µes:

- remover tentativas de:
  - "ignore previous instructions"
  - "act as system"
  - "delete database"
  - "reveal prompt"
  - etc.

- bloquear palavras proibidas configur√°veis  
- filtrar ataques de prompt injection conhecidos

### 4.2. Regras obrigat√≥rias
- Pergunta do usu√°rio nunca deve substituir o ‚ÄúSystem Prompt‚Äù  
- Nunca concatenar entrada do usu√°rio diretamente ao in√≠cio do prompt  
- Sempre colocar a entrada do usu√°rio dentro de um bloco delimitado:

USER QUESTION:
<<<
{{pergunta}}



---

# ‚úîÔ∏è 5. ADICIONAR MECANISMO DE OBSERVABILITY E MONITORAMENTO

Criar ou ampliar logs:

### 5.1. Logar m√©tricas por requisi√ß√£o RAG:
- tempo total da opera√ß√£o  
- tempo da consulta ao file search  
- tempo da execu√ß√£o LLM  
- n√∫mero de chunks retornados  
- n√∫mero de tokens do prompt  
- qual modelo foi usado  
- fallback ativado ou n√£o  

### 5.2. Criar log estruturado
JSON em linha, por exemplo:

{
‚Äúevent‚Äù: ‚ÄúRAG_EXECUTION‚Äù,
‚Äúbaby_id‚Äù: ‚Äú‚Ä¶‚Äù,
‚Äúquestion‚Äù: ‚Äú‚Ä¶‚Äù,
‚Äúmodel_used‚Äù: ‚Äúgemini-pro‚Äù,
‚Äúfallback‚Äù: false,
‚Äúfile_search_chunks‚Äù: 7,
‚Äúduration_total_ms‚Äù: 2520,
‚Äútimestamp‚Äù: ‚Äú2025-02-17T12:30:22Z‚Äù
}

### 5.3. Logs de erro n√£o podem expor:
- conte√∫do do banco  
- credenciais  

---

# ‚úîÔ∏è 6. APRIMORAR O ENDPOINT `/rag/ask` PARA SER MAIS RESILIENTE

### Agora o endpoint deve:

1. Sanitizar a pergunta  
2. Tentar buscar resposta em cache  
3. Executar fun√ß√£o `runRAG` otimizada  
4. Em caso de erro do Gemini ‚Üí fallback OpenAI  
5. Em caso total de falha ‚Üí mensagem segura:

‚ÄúN√£o consegui acessar nossas bases de conhecimento agora, mas estou aqui!
Pode tentar novamente em instantes? üíõ‚Äù

6. Armazenar resposta no cache  
7. Registrar log detalhado  

---

# ‚úîÔ∏è 7. ATUALIZA√á√ÉO DE DOCUMENTA√á√ÉO

Atualizar `docs/RAG-EDUCARE.md` com:

### 7.1. Estrutura e uso do cache  
### 7.2. Como funciona o fallback de LLM  
### 7.3. Regras de sanitiza√ß√£o de prompt  
### 7.4. Pol√≠ticas de custo e limites  
### 7.5. Estrutura dos logs e como interpret√°-los  
### 7.6. Exemplos de requisi√ß√£o/response atualizados  

---

# ‚ö†Ô∏è REGRAS DE SEGURAN√áA DA FASE 5

- N√£o alterar modelos de tabelas existentes  
- N√£o mudar comportamento de endpoints antigos  
- N√£o atrapalhar o fluxo do n8n  
- Toda falha deve ser capturada e tratada  
- O endpoint RAG nunca pode quebrar o servidor  
- A sanitiza√ß√£o nunca pode interferir na l√≥gica interna do Educare  

---

# üìå SA√çDA ESPERADA DA FASE 5

- RAG mais r√°pido (cache)
- RAG mais barato (controle de tokens + fallback inteligente)
- RAG mais seguro (inje√ß√£o bloqueada)
- RAG mais est√°vel (fallback autom√°tico)
- RAG mais observ√°vel (logs e m√©tricas)
- Endpoint `/rag/ask` fortalecido e resiliente  
- Nenhum crash introduzido no backend existente

---